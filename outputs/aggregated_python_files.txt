

===== validator.py =====

# validator.py

import os
import json
import logging
from typing import Dict, Any, List, Tuple, Optional, Set
import difflib
import re  # Import regex module for text normalization
import math  # Import math module for NaN checks

# Import FlexibleDataHandler
from transcriptanalysis.data_handlers import FlexibleDataHandler

# Configure logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)  # Set to DEBUG for detailed logs; adjust as needed

# Create console handler with a higher log level
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)  # Adjust as needed

# Create formatter and add it to the handlers
formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(name)s: %(message)s')
ch.setFormatter(formatter)

# Add the handlers to the logger
if not logger.handlers:
    logger.addHandler(ch)


def replace_nan_with_null(obj):
    """
    Recursively replace NaN values with None in a data structure.
    """
    if isinstance(obj, float):
        if math.isnan(obj):
            return None
        else:
            return obj
    elif isinstance(obj, dict):
        return {k: replace_nan_with_null(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [replace_nan_with_null(v) for v in obj]
    else:
        return obj


def normalize_text(text: str) -> str:
    """
    Normalizes text by removing extra spaces and newline characters.
    Converts all whitespace sequences to a single space and strips leading/trailing spaces.

    Args:
        text (str): The text to normalize.

    Returns:
        str: The normalized text.
    """
    # Replace any sequence of whitespace characters with a single space
    normalized = re.sub(r'\s+', ' ', text).strip()
    return normalized


def compare_texts(original: str, concatenated: str) -> Tuple[bool, str]:
    """
    Compares two texts after normalizing them by ignoring extra spaces and newline characters.
    Returns whether they are identical and the diff.

    Args:
        original (str): Original preliminary segment text.
        concatenated (str): Concatenated meaning units text.

    Returns:
        Tuple[bool, str]: (is_identical, diff_string)
    """
    normalized_original = normalize_text(original)
    normalized_concatenated = normalize_text(concatenated)

    is_identical = normalized_original == normalized_concatenated
    if is_identical:
        return True, ""
    else:
        # Generate a unified diff based on normalized texts
        original_lines = normalized_original.splitlines()
        concatenated_lines = normalized_concatenated.splitlines()
        diff = difflib.unified_diff(
            original_lines,
            concatenated_lines,
            fromfile='Original Preliminary Segment',
            tofile='Concatenated Meaning Units',
            lineterm=''
        )
        diff_string = '\n'.join(diff)
        return False, diff_string


def generate_report(
    preliminary_segments: Dict[str, Dict[str, Any]],
    meaning_units: Dict[str, List[Dict[str, Any]]],
    similarity_threshold: float = 1.0,
    report_file: str = 'validation_report.json',
    text_field: str = 'text',
    source_id_field: Optional[str] = None,
    meaning_unit_uuid_field: Optional[str] = None,  # NEW: Field name for meaning_unit_uuid
    filtered_source_ids: Optional[Set[str]] = None  # New parameter
) -> Dict[str, Any]:
    """
    Generates a report of inconsistencies and missing meaning units.

    Args:
        preliminary_segments (Dict[str, Dict[str, Any]]): Mapping from source_id to preliminary segment data.
        meaning_units (Dict[str, List[Dict[str, Any]]]): Mapping from source_id to list of meaning units.
        similarity_threshold (float, optional): Threshold for similarity. Defaults to 1.0 for exact match.
        report_file (str, optional): Path to save the validation report JSON file. Defaults to 'validation_report.json'.
        text_field (str, optional): The field name that contains the preliminary segment text.
        source_id_field (Optional[str], optional): The field name that contains the source_id.
        meaning_unit_uuid_field (Optional[str], optional): The field name that contains the meaning_unit_uuid.
        filtered_source_ids (Optional[Set[str]], optional): Set of source_ids that were filtered out.

    Returns:
        Dict[str, Any]: Report containing skipped and inconsistent preliminary segments.
    """
    total_preliminary_segments = len(preliminary_segments)
    total_meaning_units = sum(len(units) for units in meaning_units.values())
    logger.info(f"Total preliminary segments: {total_preliminary_segments}")
    logger.info(f"Total meaning units: {total_meaning_units}")

    skipped_preliminary_segments = []
    inconsistent_preliminary_segments = []

    for source_id, segment in preliminary_segments.items():
        # Skip if the preliminary segment was filtered out
        if filtered_source_ids and source_id in filtered_source_ids:
            continue

        original_text = segment.get(text_field, '').strip()
        units = meaning_units.get(source_id, [])

        if not units:
            skipped_preliminary_segments.append({
                'source_id': source_id,
                'preliminary_segment_text': original_text,
                'metadata': {k: v for k, v in segment.items() if k != text_field}
            })
            continue

        # Concatenate meaning unit strings in order
        concatenated_text = ' '.join(unit.get('meaning_unit_string', '').strip() for unit in units)

        is_identical, diff = compare_texts(original_text, concatenated_text)

        if not is_identical:
            # Collect all meaning_unit_uuids associated with this source_id
            meaning_unit_uuids = [unit.get(meaning_unit_uuid_field, "N/A") for unit in units if meaning_unit_uuid_field and unit.get(meaning_unit_uuid_field)]

            inconsistent_preliminary_segments.append({
                'source_id': source_id,
                'meaning_unit_uuids': meaning_unit_uuids,  # NEW: Include all UUIDs
                'preliminary_segment_text': original_text,
                'concatenated_meaning_units_text': concatenated_text,
                'diff': diff,
                'metadata': {k: v for k, v in segment.items() if k != text_field}
            })

    # Prepare the report
    report = {
        'total_preliminary_segments': total_preliminary_segments,
        'total_meaning_units': total_meaning_units,
        'skipped_preliminary_segments': skipped_preliminary_segments,
        'inconsistent_preliminary_segments': inconsistent_preliminary_segments
    }

    # Replace NaN values with null
    report = replace_nan_with_null(report)

    # Save the report to a JSON file
    try:
        logger.debug(f"Saving validation report to '{report_file}'.")
        with open(report_file, 'w', encoding='utf-8') as outfile:
            json.dump(report, outfile, indent=2)
        logger.info(f"Validation report saved to '{report_file}'.")
    except Exception as e:
        logger.error(f"Error saving validation report to '{report_file}': {e}")
        raise e

    return report


def run_validation(
    input_file: str,
    output_file: str,
    report_file: str = 'validation_report.json',
    similarity_threshold: float = 1.0,
    input_list_field: Optional[str] = None,
    output_list_field: Optional[str] = None,
    text_field: str = 'text',
    source_id_field: Optional[str] = None,
    meaning_unit_uuid_field: Optional[str] = None,  # NEW: Field name for meaning_unit_uuid
    filter_rules: Optional[List[Dict[str, Any]]] = None,  # Ensure filter_rules can be passed
    context_fields: Optional[List[str]] = None,
    use_parsing: bool = True,
    parse_instructions: str = '',
    completion_model: str = 'gpt-4o-mini',
    preliminary_segments_per_prompt: int = 1  # Renamed from speaking_turns_per_prompt
) -> Dict[str, Any]:
    """
    Runs the validation process.

    Args:
        input_file (str): Path to the input JSON file containing preliminary segments.
        output_file (str): Path to the output JSON file containing meaning units.
        report_file (str, optional): Path to save the validation report JSON file. Defaults to 'validation_report.json'.
        similarity_threshold (float, optional): Threshold for similarity. Defaults to 1.0 for exact match.
        input_list_field (Optional[str], optional): Dot-separated path to the list of items within the input JSON. Defaults to None.
        output_list_field (Optional[str], optional): Dot-separated path to the list of items within the output JSON.
        text_field (str, optional): The field name that contains the preliminary segment text.
        source_id_field (Optional[str], optional): The field name that contains the source_id.
        meaning_unit_uuid_field (Optional[str], optional): The field name that contains the meaning_unit_uuid.
        filter_rules (Optional[List[Dict[str, Any]]], optional): List of filter rules to apply.
        context_fields (Optional[List[str]], optional): List of fields to include as context.
        use_parsing (bool, optional): Whether parsing was used in the main processing.
        parse_instructions (str, optional): Parse instructions used in the main processing.
        completion_model (str, optional): Completion model used for parsing.
        preliminary_segments_per_prompt (int, optional): Number of preliminary segments per parsing prompt.

    Returns:
        Dict[str, Any]: The validation report.
    """
    # Load input data using FlexibleDataHandler
    try:
        data_handler = FlexibleDataHandler(
            file_path=input_file,
            parse_instructions=parse_instructions,
            completion_model=completion_model,
            content_field=text_field,
            context_fields=context_fields,
            list_field=input_list_field,
            filter_rules=filter_rules,  # Pass the filter_rules here
            use_parsing=False,  # We don't need to parse again
            source_id_field=source_id_field,
            preliminary_segments_per_prompt=preliminary_segments_per_prompt  # Renamed
        )
        data_df = data_handler.load_data()
        logger.debug(f"Loaded data with shape {data_df.shape}.")
        filtered_out_source_ids = data_handler.filtered_out_source_ids  # Get filtered out source_ids
    except Exception as e:
        logger.error(f"Data loading failed: {e}")
        raise e

    # Use the full_data to include all preliminary_segments in the report
    preliminary_segments = data_handler.full_data.set_index('source_id').to_dict(orient='index')

    # Load meaning units from output file
    meaning_units = load_output_file(output_file, list_field=output_list_field)

    # Generate report
    report = generate_report(
        preliminary_segments,
        meaning_units,
        similarity_threshold=similarity_threshold,
        report_file=report_file,
        text_field=text_field,
        source_id_field=source_id_field,
        meaning_unit_uuid_field=meaning_unit_uuid_field,  # NEW: Pass meaning_unit_uuid_field
        filtered_source_ids=filtered_out_source_ids  # Pass filtered source_ids
    )

    return report


def load_output_file(
    output_file_path: str,
    list_field: Optional[str] = None
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Loads the output file and returns a dictionary mapping source_id to a list of meaning units.

    Args:
        output_file_path (str): Path to the output JSON file.
        list_field (Optional[str]): Dot-separated path to the list of items within the JSON.

    Returns:
        Dict[str, List[Dict[str, Any]]]: Mapping from source_id to list of meaning units.
    """
    data = load_json_file(output_file_path, list_field=list_field)

    # If no list_field is provided, assume the list is directly under a key (e.g., "meaning_units")
    if not list_field and isinstance(data, dict):
        data = data.get('meaning_units', [])

    meaning_units = {}
    for unit in data:
        preliminary_segment = unit.get('preliminary_segment', {})
        source_id = preliminary_segment.get('source_id')
        if source_id is None:
            logger.warning(f"Skipping meaning unit without 'source_id' in preliminary_segment: {unit}")
            continue
        if source_id not in meaning_units:
            meaning_units[source_id] = []
        meaning_units[source_id].append(unit)
    logger.info(f"Loaded meaning units derived from {len(meaning_units)} unique source_ids.")
    return meaning_units


def load_json_file(
    file_path: str,
    list_field: Optional[str] = None
) -> Any:
    """
    Loads a JSON file and optionally navigates to a list of items using list_field.

    Args:
        file_path (str): Path to the JSON file.
        list_field (Optional[str]): Dot-separated path to the list of items within the JSON.

    Returns:
        Any: The data loaded from the JSON file, possibly after navigating to the list_field.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as infile:
            data = json.load(infile)
    except Exception as e:
        logger.error(f"Error loading JSON file '{file_path}': {e}")
        raise e

    # Navigate to the list of items using list_field if provided
    if list_field:
        keys = list_field.split('.')
        for key in keys:
            if isinstance(data, dict):
                data = data.get(key, [])
            else:
                logger.error(f"Expected dict while accessing '{key}' in 'list_field', but got {type(data)}")
                data = []
                break
    return data


===== logging_config.py =====

# logging_config.py

import logging
from pathlib import Path

def setup_logging(
    enable_logging: bool,
    logging_level_str: str = "DEBUG",
    log_to_file: bool = True,
    log_file_path: str = "logs/application.log"
) -> None:
    """
    Centralized logging configuration function.

    Args:
        enable_logging (bool): Whether to enable logging or set to CRITICAL only.
        logging_level_str (str): Logging level as a string, e.g. 'DEBUG' or 'INFO'.
        log_to_file (bool): If True, logs will also be written to a file.
        log_file_path (str): File path for the log file.
    """
    if enable_logging:
        # Convert string level to actual logging level (default to DEBUG if invalid)
        logging_level = getattr(logging, logging_level_str.upper(), logging.DEBUG)

        handlers = [logging.StreamHandler()]
        if log_to_file:
            log_file_path_obj = Path(log_file_path)
            log_file_path_obj.parent.mkdir(parents=True, exist_ok=True)
            handlers.append(logging.FileHandler(log_file_path_obj))

        logging.basicConfig(
            level=logging_level,
            format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
            handlers=handlers
        )
    else:
        # If logging is disabled, set level to CRITICAL so minimal logs appear
        logging.basicConfig(level=logging.CRITICAL)


===== config_schemas.py =====

# config_schemas.py

from typing import Optional, Dict, Any, List
from enum import Enum
from pydantic import BaseModel, field_validator, model_validator, RootModel

# ---------------------------
# Enums
# ---------------------------

class OperatorEnum(str, Enum):
    EQUALS = "equals"
    NOT_EQUALS = "not_equals"
    GREATER_THAN = "greater_than"
    LESS_THAN = "less_than"
    CONTAINS = "contains"
    # Add other operators as needed

class CodingModeEnum(str, Enum):
    DEDUCTIVE = "deductive"
    INDUCTIVE = "inductive"

class LoggingLevelEnum(str, Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

# ---------------------------
# New Enums & Models for LLM Configuration
# ---------------------------

class ProviderEnum(str, Enum):
    OPENAI = "openai"
    HUGGINGFACE = "huggingface"

class LLMConfig(BaseModel):
    provider: ProviderEnum
    model_name: str
    temperature: float = 0.7
    max_tokens: int = 2000
    api_key: Optional[str] = None

# ---------------------------
# Data Format Config Models
# ---------------------------

class FilterRule(BaseModel):
    field: str
    operator: OperatorEnum
    value: str

class DataFormatConfigItem(BaseModel):
    """
    Replaces the old 'speaker_field' with 'context_fields', allowing multiple fields
    from the JSON to be injected as context per preliminary segment.
    """
    content_field: str
    context_fields: Optional[List[str]] = None  # CHANGED
    list_field: Optional[str] = None
    source_id_field: Optional[str] = None
    filter_rules: List[FilterRule] = []

    @model_validator(mode='after')
    def check_required_fields(cls, values):
        """
        We only keep a check for 'movie_script' requiring 'list_field'.
        The old requirement for speaker_field is removed.
        """
        if values.content_field == 'movie_script' and not values.list_field:
            raise ValueError("list_field is required for movie_script data_format")
        return values

class DataFormatConfig(RootModel[Dict[str, DataFormatConfigItem]]):
    """
    A RootModel where each top-level key (e.g. "transcript", "movie_script")
    maps to a DataFormatConfigItem.
    """

    def __getitem__(self, item: str) -> DataFormatConfigItem:
        return self.root[item]

    def __contains__(self, item: str) -> bool:
        return item in self.root

# ---------------------------
# Main Config Models
# ---------------------------

class PathsModel(BaseModel):
    prompts_folder: str
    codebase_folder: str
    json_folder: str
    config_folder: str

class ConfigModel(BaseModel):
    coding_mode: CodingModeEnum
    use_parsing: bool
    preliminary_segments_per_prompt: int  # Renamed from speaking_turns_per_prompt
    meaning_units_per_assignment_prompt: int
    context_size: int
    data_format: str
    paths: PathsModel
    selected_codebase: str
    selected_json_file: str
    parse_prompt_file: str
    inductive_coding_prompt_file: str
    deductive_coding_prompt_file: str
    output_folder: str
    enable_logging: bool
    logging_level: LoggingLevelEnum
    log_to_file: bool
    log_file_path: str

    # NEW FIELD: specify how many threads (concurrent requests) to use
    thread_count: int = 1

    # NEW FIELDS: Separate LLM configurations for parse and assign tasks
    parse_llm_config: LLMConfig
    assign_llm_config: LLMConfig

    @field_validator('data_format')
    def validate_data_format(cls, v):
        allowed_formats = ['transcript', 'movie_script', 'other_format']  # Update as needed
        if v not in allowed_formats:
            raise ValueError(f"'data_format' must be one of {allowed_formats}, got '{v}'")
        return v

# Example Usage
if __name__ == "__main__":
    try:
        config = ConfigModel(
            coding_mode="deductive",
            use_parsing=True,
            preliminary_segments_per_prompt=5,  # Updated field name
            meaning_units_per_assignment_prompt=10,
            context_size=2048,
            data_format="transcript",
            paths={
                "prompts_folder": "/path/to/prompts",
                "codebase_folder": "/path/to/codebase",
                "json_folder": "/path/to/json",
                "config_folder": "/path/to/config"
            },
            selected_codebase="default",
            selected_json_file="data.json",
            parse_prompt_file="parse_prompt.txt",
            inductive_coding_prompt_file="inductive_prompt.txt",
            deductive_coding_prompt_file="deductive_prompt.txt",
            output_folder="/path/to/output",
            enable_logging=True,
            logging_level="INFO",
            log_to_file=True,
            log_file_path="/path/to/logfile.log",
            thread_count=4,  # Example: 4 concurrent requests
            parse_llm_config={
                "provider": "openai",
                "model_name": "gpt-4",
                "temperature": 0.7,
                "max_tokens": 2000,
                "api_key": "YOUR_OPENAI_API_KEY_FOR_PARSE"
            },
            assign_llm_config={
                "provider": "huggingface",
                "model_name": "gpt2",
                "temperature": 0.6,
                "max_tokens": 1500,
                "api_key": "YOUR_HUGGINGFACE_API_KEY_IF_NEEDED"
            }
        )
        print("Configuration loaded successfully.")
    except Exception as e:
        print(f"Error loading configuration: {e}")


===== __init__.py =====



===== api.py =====

from fastapi import FastAPI, HTTPException
from my_package.main import some_function  # Import your package's functionalities

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "Welcome to MyPackage API!"}

@app.get("/process/{input_value}")
def process_value(input_value: str):
    """
    Endpoint to process input using a function from your package.
    """
    try:
        result = some_function(input_value)  # Replace with your package function
        return {"input": input_value, "result": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error processing input: {e}")


===== utils.py =====

# transcriptanalysis/utils.py

import os
import json
import logging
from pathlib import Path
from typing import Dict, Any
from pydantic import ValidationError

from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

from .config_schemas import ConfigModel, DataFormatConfig
from .langchain_llm import LangChainLLM

from importlib import resources  # Import importlib.resources

logger = logging.getLogger(__name__)

class ParseResponse(BaseModel):
    source_id: str
    quote: str

# Instantiate a Pydantic parser for structured outputs
parser = PydanticOutputParser(pydantic_object=ParseResponse)

def load_environment_variables() -> Dict[str, str]:
    """
    Loads and validates required environment variables, returning them in a dictionary.
    For example, you could load your provider API keys here if not given in config.
    """
    openai_api_key = os.getenv('OPENAI_API_KEY', '')
    # You might also have HF API keys or other environment variables
    return {
        "OPENAI_API_KEY": openai_api_key
    }

def _load_json_file(file_path: str) -> Any:
    """
    Internal helper function to load JSON content from a file.
    """
    path = Path(file_path)
    if not path.exists():
        logger.error(f"File '{file_path}' not found.")
        raise FileNotFoundError(f"File '{file_path}' not found.")

    try:
        with path.open('r', encoding='utf-8') as file:
            data = json.load(file)
        return data
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from '{file_path}': {e}")
        raise
    except IOError as e:
        logger.error(f"I/O error loading file '{file_path}': {e}")
        raise

def _load_text_file(file_path: str, description: str = 'file') -> str:
    """
    Internal helper function to load raw text content from a file.
    """
    path = Path(file_path)
    if not path.exists():
        logger.error(f"{description.capitalize()} file '{file_path}' not found.")
        raise FileNotFoundError(f"{description.capitalize()} file '{file_path}' not found.")

    try:
        with path.open('r', encoding='utf-8') as file:
            content = file.read().strip()
    except OSError as e:
        logger.error(f"Error reading {description} file '{file_path}': {e}")
        raise

    if not content:
        logger.error(f"{description.capitalize()} file '{file_path}' is empty.")
        raise ValueError(f"{description.capitalize()} file '{file_path}' is empty.")

    return content

def load_prompt_file(package: str, filename: str, description: str = 'prompt') -> str:
    """
    Loads a prompt from a specified package and filename using importlib.resources.
    
    Args:
        package (str): The package path (e.g., 'transcriptanalysis.prompts').
        filename (str): The name of the prompt file (e.g., 'parse.txt').
        description (str): A description of the file for error messages.
    
    Returns:
        str: The contents of the prompt file.
    
    Raises:
        FileNotFoundError: If the prompt file is not found within the package.
        Exception: If any other error occurs while loading the prompt file.
    """
    try:
        return resources.read_text(package, filename)
    except FileNotFoundError:
        raise FileNotFoundError(f"{description.capitalize()} file '{package}/{filename}' not found.")
    except Exception as e:
        raise Exception(f"An error occurred while loading {description} file '{package}/{filename}': {e}")

def load_config(config_file_path: str) -> ConfigModel:
    """
    Loads and validates the main configuration from a JSON file using Pydantic.
    """
    raw_config = _load_json_file(config_file_path)
    return ConfigModel.model_validate(raw_config)

def load_data_format_config(config_file_path: str) -> DataFormatConfig:
    """
    Loads and validates the data format configuration from a JSON file using Pydantic.
    """
    raw_config = _load_json_file(config_file_path)
    return DataFormatConfig.model_validate(raw_config)

def generate_structured_response(llm: LangChainLLM, prompt: str) -> Dict[str, Any]:
    """
    Example function that uses an LLM to produce structured data validated by Pydantic.
    """
    raw_text = llm.generate(prompt)
    try:
        parsed_obj = parser.parse(raw_text)
        return parsed_obj.dict()
    except ValidationError as ve:
        logger.error(f"Failed to parse structured output: {ve}")
        return {}


===== main.py =====

# transcriptanalysis/main.py

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Optional
import sys

from importlib import resources

from .logging_config import setup_logging  # Relative import
from .config_schemas import ConfigModel, DataFormatConfig
from .utils import (
    load_environment_variables,
    load_config,
    load_data_format_config,
    load_prompt_file
)
from .data_handlers import FlexibleDataHandler
from .qual_functions import assign_codes_to_meaning_units
from .validator import run_validation, replace_nan_with_null
from .langchain_llm import LangChainLLM


def main(config: ConfigModel):
    """
    Main function to execute the qualitative coding pipeline.
    """
    setup_logging(
        enable_logging=config.enable_logging,
        logging_level_str=config.logging_level,
        log_to_file=config.log_to_file,
        log_file_path=config.log_file_path
    )
    logger = logging.getLogger(__name__)

    logger.info("Starting the main pipeline.")

    # Stage 1: Environment Setup
    env_vars = load_environment_variables()
    if not config.parse_llm_config.api_key and env_vars.get("OPENAI_API_KEY"):
        config.parse_llm_config.api_key = env_vars["OPENAI_API_KEY"]
    if not config.assign_llm_config.api_key and env_vars.get("HUGGINGFACE_API_KEY"):
        config.assign_llm_config.api_key = env_vars["HUGGINGFACE_API_KEY"]

    parse_instructions = ""
    if config.use_parsing:
        parse_instructions = load_prompt_file(
            'transcriptanalysis.prompts',      # Package path
            config.parse_prompt_file,          # Filename, e.g., 'parse.txt'
            description='parse instructions'
        )

    # Load data format configuration using importlib.resources
    try:
        with resources.path('transcriptanalysis.configs', 'data_format_config.json') as data_format_config_path:
            data_format_config: DataFormatConfig = load_data_format_config(str(data_format_config_path))
    except FileNotFoundError:
        logger.error("File 'transcriptanalysis.configs/data_format_config.json' not found.")
        raise

    if config.data_format not in data_format_config:
        logger.error(f"No configuration found for data format: {config.data_format}")
        raise ValueError(f"No configuration found for data format: {config.data_format}")

    format_config = data_format_config[config.data_format]

    # Determine the data file to load using importlib.resources
    try:
        with resources.path('transcriptanalysis.json_inputs', 'teacher_transcript.json') as data_file_path:
            file_path = Path(data_file_path)
    except FileNotFoundError:
        logger.error("File 'transcriptanalysis.json_inputs/teacher_transcript.json' not found in package resources.")
        raise

    if not file_path.exists():
        logger.error(f"Data file '{file_path}' not found.")
        raise FileNotFoundError(f"Data file '{file_path}' not found.")

    # Stage 2: Data Loading & Transform
    # CHANGED: pass context_fields instead of speaker_field
    data_handler = FlexibleDataHandler(
        file_path=str(file_path),
        parse_instructions=parse_instructions,
        completion_model=config.parse_llm_config.model_name,  # Use parse LLM config
        content_field=format_config.content_field,
        context_fields=format_config.context_fields,  # CHANGED
        list_field=format_config.list_field,
        source_id_field=format_config.source_id_field,
        filter_rules=[rule.model_dump() for rule in format_config.filter_rules],
        use_parsing=config.use_parsing,
        preliminary_segments_per_prompt=config.preliminary_segments_per_prompt,  # Renamed
        thread_count=config.thread_count
    )
    data_df = data_handler.load_data()
    meaning_unit_object_list = data_handler.transform_data(data_df)

    if not meaning_unit_object_list:
        logger.warning("No meaning units to process. Exiting.")
        return

    # Stage 3: Code Assignment
    if config.coding_mode == "deductive":
        coding_instructions = load_prompt_file(
            'transcriptanalysis.prompts',  # Package path
            config.deductive_coding_prompt_file,  # Filename, e.g., 'deductive_coding.txt'
            description='deductive coding prompt'
        )
        codebase_file = Path(config.paths.codebase_folder) / config.selected_codebase
        if not codebase_file.exists():
            logger.error(f"List of codes file '{codebase_file}' not found.")
            raise FileNotFoundError(f"List of codes file '{codebase_file}' not found.")

        with codebase_file.open('r', encoding='utf-8') as file:
            processed_codes = [json.loads(line) for line in file if line.strip()]

        assign_llm = LangChainLLM(config.assign_llm_config)

        coded_meaning_unit_list = assign_codes_to_meaning_units(
            meaning_unit_list=meaning_unit_object_list,
            coding_instructions=coding_instructions,
            processed_codes=processed_codes,
            codebase=processed_codes,
            completion_model=config.assign_llm_config.model_name,
            context_size=config.context_size,
            meaning_units_per_assignment_prompt=config.meaning_units_per_assignment_prompt,
            context_fields=format_config.context_fields,  # CHANGED
            content_field=format_config.content_field,
            full_preliminary_segments=data_handler.full_data.to_dict(orient='records'),  # Renamed
            thread_count=config.thread_count,
            llm_config=config.assign_llm_config
        )
    else:  # inductive
        inductive_coding_prompt = load_prompt_file(
            'transcriptanalysis.prompts',  # Package path
            config.inductive_coding_prompt_file,  # Filename, e.g., 'inductive_coding.txt'
            description='inductive coding prompt'
        )
        assign_llm = LangChainLLM(config.assign_llm_config)

        coded_meaning_unit_list = assign_codes_to_meaning_units(
            meaning_unit_list=meaning_unit_object_list,
            coding_instructions=inductive_coding_prompt,
            processed_codes=None,
            codebase=None,
            completion_model=config.assign_llm_config.model_name,
            context_size=config.context_size,
            meaning_units_per_assignment_prompt=config.meaning_units_per_assignment_prompt,
            context_fields=format_config.context_fields,  # CHANGED
            content_field=format_config.content_field,
            full_preliminary_segments=data_handler.full_data.to_dict(orient='records'),  # Renamed
            thread_count=config.thread_count,
            llm_config=config.assign_llm_config
        )

    # Stage 4: Output Results
    output_folder = Path(config.output_folder)
    output_folder.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file_basename = Path(config.selected_json_file).stem
    output_file_path = output_folder / f"{output_file_basename}_output_{timestamp}.json"

    document_metadata = data_handler.document_metadata

    output_data = {
        "document_metadata": document_metadata,
        "meaning_units": [unit.to_dict() for unit in coded_meaning_unit_list]
    }
    output_data = replace_nan_with_null(output_data)

    with output_file_path.open('w', encoding='utf-8') as outfile:
        json.dump(output_data, outfile, indent=2)

    logger.info(f"Coded meaning units saved to '{output_file_path}'.")

    # Stage 5: Validation
    validation_report_filename = f"{output_file_basename}_validation_report.json"
    validation_report_path = output_folder / validation_report_filename

    run_validation(
        input_file=str(file_path),
        output_file=str(output_file_path),
        report_file=str(validation_report_path),
        similarity_threshold=1.0,
        filter_rules=[rule.model_dump() for rule in format_config.filter_rules],
        input_list_field=format_config.list_field,
        output_list_field='meaning_units',
        text_field=format_config.content_field,
        source_id_field=format_config.source_id_field,
        meaning_unit_uuid_field='meaning_unit_uuid'  # NEW: Include meaning_unit_uuid in validation
    )
    logger.info(f"Validation completed. Report saved to '{validation_report_path}'.")


def run():
    """
    Entry point for the script. Loads the configuration and invokes the main function.
    """
    try:
        # Access the config.json from the package resources using importlib.resources
        with resources.path('transcriptanalysis.configs', 'config.json') as config_path:
            # Convert Path object to string if necessary
            config: ConfigModel = load_config(str(config_path))
    except Exception as e:
        print(f"Failed to load configuration: {e}", file=sys.stderr)
        sys.exit(1)

    main(config)


if __name__ == "__main__":
    run()


===== qual_functions.py =====

# qual_functions.py

import logging
import json
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from pydantic import BaseModel, ValidationError

from transcriptanalysis.langchain_llm import LangChainLLM
from transcriptanalysis.config_schemas import LLMConfig

logger = logging.getLogger(__name__)

@dataclass
class CodeAssigned:
    code_name: str
    code_justification: str

    def is_valid(self) -> bool:
        return bool(self.code_name and self.code_justification)

@dataclass
class PreliminarySegment:
    source_id: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        metadata = self.metadata.copy()
        metadata.pop('source_id', None)
        return {
            "source_id": self.source_id,
            "content": self.content,
            "metadata": metadata
        }

@dataclass
class MeaningUnit:
    meaning_unit_id: int
    meaning_unit_uuid: str  # Unique UUID for this MeaningUnit
    source_id: str  # NEW: Explicitly link the MeaningUnit to the preliminary segment
    meaning_unit_string: str
    assigned_code_list: List[CodeAssigned] = field(default_factory=list)
    preliminary_segment: Optional[PreliminarySegment] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "meaning_unit_id": self.meaning_unit_id,
            "meaning_unit_uuid": self.meaning_unit_uuid,
            "source_id": self.source_id,  # NEW: Include source_id
            "meaning_unit_string": self.meaning_unit_string,
            "assigned_code_list": [code.__dict__ for code in self.assigned_code_list],
            "preliminary_segment": self.preliminary_segment.to_dict() if self.preliminary_segment else None
        }

# -----------------------
# MODELS FOR STRUCTURED OUTPUT
# -----------------------
class CodeAssignedModel(BaseModel):
    code_name: str
    code_justification: str

class CodeAssignmentResponse(BaseModel):
    meaning_unit_id: int
    codeList: List[CodeAssignedModel]

class CodeResponse(BaseModel):
    assignments: List[CodeAssignmentResponse]


def assign_codes_to_meaning_units(
    meaning_unit_list: List[MeaningUnit],
    coding_instructions: str,
    processed_codes: Optional[List[Dict[str, Any]]] = None,
    codebase: Optional[List[Dict[str, Any]]] = None,
    completion_model: str = "gpt-4",
    context_size: int = 2,
    meaning_units_per_assignment_prompt: int = 1,
    context_fields: Optional[List[str]] = None,  # CHANGED
    content_field: str = 'content',
    full_preliminary_segments: Optional[List[Dict[str, Any]]] = None,  # Renamed
    thread_count: int = 1,
    llm_config: Optional[LLMConfig] = None
) -> List[MeaningUnit]:
    """
    Assigns codes to each MeaningUnit, including context.
    Uses concurrency to parallelize LLM calls.
    """

    if not llm_config:
        logger.error("No LLMConfig provided.")
        return meaning_unit_list

    llm = LangChainLLM(llm_config)

    if full_preliminary_segments is None:
        logger.error("full_preliminary_segments must be provided for context.")
        return meaning_unit_list

    # Create a mapping from source_id to index for context retrieval
    source_id_to_index = {str(d.get('source_id')): idx for idx, d in enumerate(full_preliminary_segments)}

    # Prepare the codebase block
    if processed_codes:
        codes_to_include = codebase if codebase else processed_codes
        unique_codes_strs = set(json.dumps(code, indent=2, sort_keys=True) for code in codes_to_include)
        code_heading = "Full Codebase (all codes with details):"
        codes_block = f"{code_heading}\n{chr(10).join(unique_codes_strs)}\n\n"
    else:
        code_heading = "Guidelines for Inductive Coding:"
        codes_block = (
            f"{code_heading}\nNo predefined codes. Please generate codes based on the following guidelines.\n\n"
        )

    # We store results from each batch in a dict to avoid collisions
    batch_results_map: Dict[int, List[CodeAssigned]] = {}

    def process_batch(start_idx: int) -> Dict[int, List[CodeAssigned]]:
        """
        Process a batch of meaning units, returning a dict so we can safely
        merge results after threads finish.
        """
        local_result: Dict[int, List[CodeAssigned]] = {}
        batch = meaning_unit_list[start_idx:start_idx + meaning_units_per_assignment_prompt]
        if not batch:
            return local_result

        # Build a combined prompt for this batch
        full_prompt = f"{coding_instructions}\n\n{codes_block}"

        for unit in batch:
            unit_context = ""
            source_id = unit.preliminary_segment.source_id
            unit_idx = source_id_to_index.get(source_id)

            # Retrieve context_preliminary_segments up to `context_size`
            if unit_idx is not None:
                start_context_idx = max(0, unit_idx - (context_size - 1))
                end_context_idx = unit_idx + 1
                context_preliminary_segments = full_preliminary_segments[start_context_idx:end_context_idx]
                for st in context_preliminary_segments:
                    st_source_id = str(st.get('source_id'))
                    context_info_lines = []
                    if context_fields:
                        for fld in context_fields:
                            val = st.get(fld, "Unknown")
                            context_info_lines.append(f"{fld}: {val}")
                    else:
                        # fallback if no context fields
                        context_info_lines.append("No context fields defined.")

                    content_val = st.get(content_field, "")
                    context_block = f"ID: {st_source_id}\n" + "\n".join(context_info_lines) + f"\n{content_val}\n\n"
                    unit_context += context_block

            # For the current excerpt, also gather context from the meaning unit's preliminary segment
            current_context_lines = []
            if context_fields and unit.preliminary_segment:
                for fld in context_fields:
                    val = unit.preliminary_segment.metadata.get(fld, "Unknown")
                    current_context_lines.append(f"{fld}: {val}")

            current_unit_excerpt = f"Quote: {unit.meaning_unit_string}\n\n"

            full_prompt += (
                f"Contextual Excerpts for Meaning Unit ID {unit.meaning_unit_id}:\n{unit_context}\n"
                f"Current Excerpt For Coding (Meaning Unit ID {unit.meaning_unit_id}):\n"
                + "\n".join(current_context_lines) + "\n"
                + f"{current_unit_excerpt}"
            )

        if processed_codes:
            full_prompt += "**Apply codes exclusively to the excerpt(s) provided above.**\n\n"
        else:
            full_prompt += "**Generate codes based on the excerpt(s) provided above using the guidelines.**\n\n"

        # Attempt structured generation
        try:
            code_response = llm.structured_generate(full_prompt, CodeResponse)
        except Exception as e:
            logger.warning(f"Structured generation failed for batch {start_idx}, using fallback empty assignment. Error: {e}")
            code_response = CodeResponse(assignments=[])

        for assignment in code_response.assignments:
            assigned_codes_list: List[CodeAssigned] = []
            for code_item in assignment.codeList:
                assigned_codes_list.append(CodeAssigned(
                    code_name=code_item.code_name,
                    code_justification=code_item.code_justification
                ))
            local_result[assignment.meaning_unit_id] = assigned_codes_list

        return local_result

    from math import ceil
    total_batches = ceil(len(meaning_unit_list) / meaning_units_per_assignment_prompt)

    with ThreadPoolExecutor(max_workers=thread_count) as executor:
        futures = []
        for i in range(0, len(meaning_unit_list), meaning_units_per_assignment_prompt):
            futures.append(executor.submit(process_batch, i))

        for future in as_completed(futures):
            batch_dict = future.result()
            for mu_id, code_list in batch_dict.items():
                batch_results_map[mu_id] = code_list

    # Update meaning_unit_list with assigned codes
    for mu in meaning_unit_list:
        if mu.meaning_unit_id in batch_results_map:
            mu.assigned_code_list = batch_results_map[mu.meaning_unit_id]

    return meaning_unit_list


===== langchain_llm.py =====

# langchain_llm.py

from typing import Any, Type, Union, Dict
from langchain_openai import ChatOpenAI
from huggingface_hub import InferenceClient
from pydantic import BaseModel
from transcriptanalysis.config_schemas import LLMConfig, ProviderEnum

"""
CHANGES:
1) Added a `structured_generate` method that attempts to call `with_structured_output(...)`
   if using an OpenAI model that supports it. Otherwise, it falls back to a manual JSON approach.
"""

class LangChainLLM:
    """
    A simple wrapper around LangChain LLMs to abstract provider details (OpenAI / Hugging Face).
    """
    def __init__(self, config: LLMConfig):
        self.config = config
        self.provider = config.provider
        self.client = self._initialize_client(config)

    def _initialize_client(self, config: LLMConfig) -> Any:
        if self.provider == ProviderEnum.OPENAI:
            # ChatOpenAI from langchain_openai, passing in OpenAI API key
            return ChatOpenAI(
                model_name=config.model_name,
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                openai_api_key=config.api_key
            )
        elif self.provider == ProviderEnum.HUGGINGFACE:
            # Hugging Face Serverless Inference API
            if not config.api_key:
                raise ValueError("Hugging Face API key is required for serverless inference.")
            return InferenceClient(
                model=config.model_name,
                token=config.api_key
            )
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

    def generate(self, prompt: str) -> str:
        """
        Generate text completion from the underlying client.
        """
        try:
            if self.provider == ProviderEnum.OPENAI:
                # Generate using ChatOpenAI
                response = self.client.generate([prompt])
                return response.generations[0][0].text
            elif self.provider == ProviderEnum.HUGGINGFACE:
                # Generate using Hugging Face InferenceClient
                response = self.client.chat.completions.create(prompt)
                return response.choices[0].message
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
        except Exception as e:
            # Handle exceptions and provide meaningful error messages
            raise RuntimeError(f"An error occurred during text generation: {e}")

    # ---------------------------------------------------------------
    # NEW: structured_generate method using with_structured_output
    # ---------------------------------------------------------------
    def structured_generate(self, prompt: str, schema: Type[BaseModel]) -> BaseModel:
        """
        Attempt to generate structured data from the model, adhering to the given Pydantic schema.
        - If using OpenAI with a model that supports tool/function calling or JSON Mode, 
          we leverage .with_structured_output().
        - Otherwise, we fallback to manual JSON parsing of the LLM output.
        """
        if self.provider in {ProviderEnum.OPENAI, ProviderEnum.HUGGINGFACE}:
            try:
                structured_llm = self.client.with_structured_output(schema)
                result = structured_llm.invoke(prompt)
                return result  # result is an instance of `schema`
            except AttributeError:
                # Fallback to manual
                raw_text = self.generate(prompt)
                return self._manual_parse(raw_text, schema)
        else:
            # prompt for JSON and parse
            raw_text = self.generate(prompt)
            return self._manual_parse(raw_text, schema)

    def _manual_parse(self, raw_text: str, schema: Type[BaseModel]) -> BaseModel:
        """
        Parse the raw text as JSON and validate against the given schema.
        If parsing fails, return an empty instance of the schema.
        """
        try:
            data = self._extract_first_json(raw_text)
            return schema.model_validate(data)
        except Exception:
            # In case of failure, return an empty instance
            return schema()  # type: ignore

    def _extract_first_json(self, text: str) -> Dict[str, Any]:
        """
        Very naive approach to extract the first valid JSON block from text.
        """
        import json, re
        pattern = r"\{(?:[^{}]|(?R))*\}"
        matches = re.findall(pattern, text)
        for match in matches:
            try:
                return json.loads(match)
            except json.JSONDecodeError:
                continue
        # If no match or all fail
        return {}


===== data_handlers.py =====

# data_handlers.py

import json
import logging
import os
import uuid  # Import UUID module
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

from transcriptanalysis.qual_functions import MeaningUnit, PreliminarySegment
from transcriptanalysis.config_schemas import ProviderEnum, LLMConfig
from transcriptanalysis.langchain_llm import LangChainLLM

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# NEW: Pydantic models for structured parse responses
# ------------------------------------------------------------------
from pydantic import BaseModel, ValidationError
from typing import List


class ParseUnit(BaseModel):
    """
    Represents a single meaning unit (smaller chunk) after parsing.
    """
    source_id: str
    quote: str


class FullParseResponse(BaseModel):
    """
    Represents the entire structured output from the parsing step.
    """
    parse_list: List[ParseUnit]


class FlexibleDataHandler:
    def __init__(
        self,
        file_path: str,
        parse_instructions: str,
        completion_model: str,
        content_field: str,
        context_fields: Optional[List[str]] = None,  # CHANGED
        list_field: Optional[str] = None,
        source_id_field: Optional[str] = None,
        filter_rules: Optional[List[Dict[str, Any]]] = None,
        use_parsing: bool = True,
        preliminary_segments_per_prompt: int = 1,  # Renamed
        thread_count: int = 1
    ):
        self.file_path = file_path
        self.parse_instructions = parse_instructions
        self.completion_model = completion_model
        self.content_field = content_field
        self.context_fields = context_fields
        self.list_field = list_field
        self.source_id_field = source_id_field
        self.filter_rules = filter_rules
        self.use_parsing = use_parsing
        self.preliminary_segments_per_prompt = preliminary_segments_per_prompt
        self.thread_count = thread_count
        self.document_metadata = {}  # Store document-level metadata
        self.full_data = None
        self.filtered_out_source_ids: Set[str] = set()

        # Initialize counters
        self.meaning_unit_counter = 1  # Independent counter for meaning_unit_id

        # ------------------------------------------------------------------
        # NEW: Initialize LangChainLLM for parsing (if needed)
        # ------------------------------------------------------------------
        self.llm = None
        if self.use_parsing:
            try:
                # Build an LLMConfig (example: defaulting to OpenAI with a 0.2 temperature)
                self.llm_config = LLMConfig(
                    provider=ProviderEnum.OPENAI,
                    model_name=self.completion_model,
                    temperature=0.2,
                    max_tokens=16000,
                    api_key=os.getenv('OPENAI_API_KEY', '')
                )
                self.llm = LangChainLLM(self.llm_config)
            except Exception as e:
                logger.warning(
                    f"Could not initialize LangChainLLM with model={self.completion_model}: {e}"
                )
                self.llm = None

    def load_data(self) -> pd.DataFrame:
        """
        Loads data from the JSON file into a DataFrame and applies filter rules if any.
        """
        try:
            with Path(self.file_path).open('r', encoding='utf-8') as file:
                raw_data = json.load(file)
            logger.debug(f"Loaded data from '{self.file_path}'.")
        except (OSError, json.JSONDecodeError) as e:
            logger.error(f"Failed to load or parse data from '{self.file_path}': {e}")
            raise

        # Determine how the data is structured
        if isinstance(raw_data, list):
            self.document_metadata = {}
            content_list = raw_data
        elif isinstance(raw_data, dict):
            if self.list_field:
                self.document_metadata = {
                    k: v for k, v in raw_data.items() if k != self.list_field
                }
                content_list = raw_data.get(self.list_field, [])
                if not content_list:
                    logger.error(f"No content found under the list_field '{self.list_field}'.")
                    raise ValueError(f"No content found under the list_field '{self.list_field}'.")
            else:
                self.document_metadata = {
                    k: v for k, v in raw_data.items() if not isinstance(v, list)
                }
                content_list = [v for v in raw_data.values() if isinstance(v, list)]
                if content_list:
                    content_list = content_list[0]
                else:
                    logger.error("No list of content items found in the data.")
                    raise ValueError("No list of content items found in the data.")
        else:
            logger.error(f"Unexpected data format in '{self.file_path}'. Expected dict or list.")
            raise ValueError(f"Unexpected data format in '{self.file_path}'.")

        data = pd.DataFrame(content_list)

        # Ensure we have a source_id
        if self.source_id_field and self.source_id_field in data.columns:
            data['source_id'] = data[self.source_id_field].astype(str)
            # Optionally, ensure uniqueness if necessary
        else:
            # Generate unique UUIDs for source_id
            data['source_id'] = [str(uuid.uuid4()) for _ in range(len(data))]

        self.full_data = data.copy()
        all_source_ids = set(data['source_id'])

        # Apply filter rules if any
        if self.filter_rules:
            mask = pd.Series(True, index=data.index)
            for rule in self.filter_rules:
                field = rule.get('field')
                operator = rule.get('operator', 'equals')
                value = rule.get('value')

                if field not in data.columns:
                    logger.warning(f"Field '{field}' not found in data. Skipping filter rule.")
                    continue

                if operator == 'equals':
                    mask &= (data[field] == value)
                elif operator == 'not_equals':
                    mask &= (data[field] != value)
                elif operator == 'contains':
                    mask &= data[field].astype(str).str.contains(str(value), na=False, regex=False)
                else:
                    logger.warning(f"Operator '{operator}' is not supported. Skipping this filter rule.")

            data = data[mask]
            logger.debug(f"Data shape after applying filter rules: {data.shape}")

        # Identify filtered out source_ids
        filtered_source_ids = all_source_ids - set(data['source_id'])
        self.filtered_out_source_ids = filtered_source_ids
        logger.debug(f"Data shape after loading: {data.shape}")

        return data

    def _run_langchain_parse_chunk(
        self,
        preliminary_segments: List[Dict[str, Any]],
        prompt: str
    ) -> List[Dict[str, str]]:
        """
        Uses LangChainLLM.structured_generate() to parse a chunk of preliminary_segments into smaller meaning units.
        Returns a list of dicts with keys "source_id" and "parsed_text".
        """

        if not self.llm:
            logger.error("LLM was not initialized; cannot parse.")
            return []

        # Build the combined prompt (system role + user instructions + data)
        system_content = (
            "You are a qualitative research assistant that breaks down multiple preliminary segments "
            "into smaller meaning units based on given instructions."
        )

        combined_prompt = f"""
System instructions:
{system_content}

User instructions:
{prompt}

Preliminary Segments (JSON):
{json.dumps(preliminary_segments, indent=2)}
        """

        try:
            parsed_response: FullParseResponse = self.llm.structured_generate(
                combined_prompt,
                FullParseResponse
            )

            if not parsed_response or not isinstance(parsed_response, FullParseResponse):
                logger.error("Parsed output is not an instance of FullParseResponse.")
                return []

            if not parsed_response.parse_list:
                logger.error("Parsed output is empty or missing 'parse_list'.")
                return []

            results = []
            for unit in parsed_response.parse_list:
                results.append({
                    "source_id": unit.source_id,
                    "parsed_text": unit.quote
                })

            return results

        except ValidationError as ve:
            logger.error(f"Validation error while parsing chunk: {ve}")
            return []
        except Exception as e:
            logger.error(f"An error occurred while parsing chunk: {e}")
            return []

    def _parse_chunk_of_data(
        self,
        chunk_data: pd.DataFrame,
        parse_instructions: str,
        batch_index: int
    ) -> Tuple[int, List[Dict[str, str]]]:
        """
        Helper method for parsing a chunk of data.
        Returns a tuple of (batch_index, parsed_units).
        """
        preliminary_segments_dicts = []
        for _, record in chunk_data.iterrows():
            preliminary_segments_dicts.append({
                "source_id": str(record['source_id']),
                "content": record.get(self.content_field, ""),
                "metadata": record.drop(labels=[self.content_field], errors='ignore').to_dict()
            })

        parsed_units = self._run_langchain_parse_chunk(
            preliminary_segments=preliminary_segments_dicts,
            prompt=parse_instructions
        )

        return (batch_index, parsed_units)

    def transform_data(self, data: pd.DataFrame) -> List[MeaningUnit]:
        """
        Transforms data into MeaningUnit objects, optionally using LLM-based parsing.
        If parsing is off, treat each row as a single meaning unit.
        Otherwise, parse in batches using concurrency.
        """
        meaning_units: List[MeaningUnit] = []

        if not self.use_parsing:
            # No parsing needed
            for _, record in data.iterrows():
                content = record.get(self.content_field, "")
                metadata = record.drop(labels=[self.content_field], errors='ignore').to_dict()
                source_id = str(record['source_id'])

                # Generate UUID for meaning_unit_uuid
                meaning_unit_uuid = str(uuid.uuid4())

                preliminary_segment = PreliminarySegment(
                    source_id=source_id,
                    content=content,
                    metadata=metadata
                )
                mu = MeaningUnit(
                    meaning_unit_id=self.meaning_unit_counter,
                    meaning_unit_uuid=meaning_unit_uuid,
                    source_id=source_id,  # NEW: Link meaning unit to the same source_id
                    meaning_unit_string=content,
                    assigned_code_list=[],
                    preliminary_segment=preliminary_segment
                )
                meaning_units.append(mu)
                self.meaning_unit_counter += 1

            logger.debug(f"Transformed data (no parsing) into {len(meaning_units)} meaning units.")
            return meaning_units

        # PARSING is ON
        chunked_data = [
            data.iloc[i: i + self.preliminary_segments_per_prompt]
            for i in range(0, len(data), self.preliminary_segments_per_prompt)
        ]

        all_parsed_results: List[Tuple[int, List[Dict[str, str]]]] = []
        with ThreadPoolExecutor(max_workers=self.thread_count) as executor:
            futures = {}
            for idx, chunk in enumerate(chunked_data):
                future = executor.submit(
                    self._parse_chunk_of_data,
                    chunk_data=chunk,
                    parse_instructions=self.parse_instructions,
                    batch_index=idx
                )
                futures[future] = idx

            for future in as_completed(futures):
                batch_index, parsed_list = future.result()
                all_parsed_results.append((batch_index, parsed_list))

        # Sort the results based on batch_index to maintain order
        all_parsed_results.sort(key=lambda x: x[0])

        for _, parsed_list in all_parsed_results:
            for item in parsed_list:
                sid = item["source_id"]
                parsed_text = item["parsed_text"]
                # Retrieve the original record to get metadata
                matching_records = data[data['source_id'] == sid]
                if not matching_records.empty:
                    record = matching_records.iloc[0]
                    metadata = record.drop(labels=[self.content_field], errors='ignore').to_dict()
                else:
                    metadata = {}

                preliminary_segment = PreliminarySegment(
                    source_id=sid,
                    content=record.get(self.content_field, "") if not matching_records.empty else "",
                    metadata=metadata
                )

                # Generate UUID for meaning_unit_uuid
                meaning_unit_uuid = str(uuid.uuid4())

                mu = MeaningUnit(
                    meaning_unit_id=self.meaning_unit_counter,
                    meaning_unit_uuid=meaning_unit_uuid,
                    source_id=sid,  # NEW: Link meaning unit to the same source_id
                    meaning_unit_string=parsed_text,
                    assigned_code_list=[],
                    preliminary_segment=preliminary_segment
                )
                meaning_units.append(mu)
                self.meaning_unit_counter += 1

        logger.debug(f"Transformed data (with parsing) into {len(meaning_units)} meaning units.")
        return meaning_units
