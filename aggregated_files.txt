

===== config_schemas.py =====

# config_schemas.py

from typing import Optional, Dict, Any, List
from enum import Enum
from pydantic import BaseModel, field_validator, model_validator, RootModel

# ---------------------------
# Enums
# ---------------------------

class OperatorEnum(str, Enum):
    EQUALS = "equals"
    NOT_EQUALS = "not_equals"
    GREATER_THAN = "greater_than"
    LESS_THAN = "less_than"
    CONTAINS = "contains"
    # Add other operators as needed

class CodingModeEnum(str, Enum):
    DEDUCTIVE = "deductive"
    INDUCTIVE = "inductive"

class LoggingLevelEnum(str, Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

# ---------------------------
# New Enums & Models for LLM Configuration
# ---------------------------

class ProviderEnum(str, Enum):
    OPENAI = "openai"
    HUGGINGFACE = "huggingface"

class LLMConfig(BaseModel):
    provider: ProviderEnum
    model_name: str
    temperature: float = 0.7
    max_tokens: int = 2000
    api_key: Optional[str] = None

# ---------------------------
# Data Format Config Models
# ---------------------------

class FilterRule(BaseModel):
    field: str
    operator: OperatorEnum
    value: str

class DataFormatConfigItem(BaseModel):
    """
    Replaces the old 'speaker_field' with 'context_fields', allowing multiple fields
    from the JSON to be injected as context per speaking turn.
    """
    content_field: str
    context_fields: Optional[List[str]] = None  # CHANGED
    list_field: Optional[str] = None
    source_id_field: Optional[str] = None
    filter_rules: List[FilterRule] = []

    @model_validator(mode='after')
    def check_required_fields(cls, values):
        """
        We only keep a check for 'movie_script' requiring 'list_field'.
        The old requirement for speaker_field is removed.
        """
        # Corrected attribute access using dot notation
        if values.content_field == 'movie_script' and not values.list_field:
            raise ValueError("list_field is required for movie_script data_format")
        return values

class DataFormatConfig(RootModel[Dict[str, DataFormatConfigItem]]):
    """
    A RootModel where each top-level key (e.g. "transcript", "movie_script") 
    maps to a DataFormatConfigItem.
    """

    def __getitem__(self, item: str) -> DataFormatConfigItem:
        return self.root[item]

    def __contains__(self, item: str) -> bool:
        return item in self.root

# ---------------------------
# Main Config Models
# ---------------------------

class PathsModel(BaseModel):
    prompts_folder: str
    codebase_folder: str
    json_folder: str
    config_folder: str

class ConfigModel(BaseModel):
    coding_mode: CodingModeEnum
    use_parsing: bool
    speaking_turns_per_prompt: int
    meaning_units_per_assignment_prompt: int
    context_size: int
    data_format: str
    paths: PathsModel
    selected_codebase: str
    selected_json_file: str
    parse_prompt_file: str
    inductive_coding_prompt_file: str
    deductive_coding_prompt_file: str
    output_folder: str
    enable_logging: bool
    logging_level: LoggingLevelEnum
    log_to_file: bool
    log_file_path: str

    # NEW FIELD: specify how many threads (concurrent requests) to use
    thread_count: int = 1

    # NEW FIELDS: Separate LLM configurations for parse and assign tasks
    parse_llm_config: LLMConfig
    assign_llm_config: LLMConfig

    @field_validator('data_format')
    def validate_data_format(cls, v):
        allowed_formats = ['transcript', 'movie_script', 'other_format']  # Update as needed
        if v not in allowed_formats:
            raise ValueError(f"'data_format' must be one of {allowed_formats}, got '{v}'")
        return v

# Example Usage
if __name__ == "__main__":
    try:
        config = ConfigModel(
            coding_mode="deductive",
            use_parsing=True,
            speaking_turns_per_prompt=5,
            meaning_units_per_assignment_prompt=10,
            context_size=2048,
            data_format="transcript",
            paths={
                "prompts_folder": "/path/to/prompts",
                "codebase_folder": "/path/to/codebase",
                "json_folder": "/path/to/json",
                "config_folder": "/path/to/config"
            },
            selected_codebase="default",
            selected_json_file="data.json",
            parse_prompt_file="parse_prompt.txt",
            inductive_coding_prompt_file="inductive_prompt.txt",
            deductive_coding_prompt_file="deductive_prompt.txt",
            output_folder="/path/to/output",
            enable_logging=True,
            logging_level="INFO",
            log_to_file=True,
            log_file_path="/path/to/logfile.log",
            thread_count=4,  # Example: 4 concurrent requests
            parse_llm_config={
                "provider": "openai",
                "model_name": "gpt-4",
                "temperature": 0.7,
                "max_tokens": 2000,
                "api_key": "YOUR_OPENAI_API_KEY_FOR_PARSE"
            },
            assign_llm_config={
                "provider": "huggingface",
                "model_name": "gpt2",
                "temperature": 0.6,
                "max_tokens": 1500,
                "api_key": "YOUR_HUGGINGFACE_API_KEY_IF_NEEDED"
            }
        )
        print("Configuration loaded successfully.")
    except Exception as e:
        print(f"Error loading configuration: {e}")


===== utils.py =====

# utils.py

import os
import json
import logging
from pathlib import Path
from typing import Dict, Any
from pydantic import ValidationError

from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

from config_schemas import ConfigModel, DataFormatConfig
from langchain_llm import LangChainLLM

logger = logging.getLogger(__name__)

class ParseResponse(BaseModel):
    source_id: str
    quote: str

# Instantiate a Pydantic parser for structured outputs
parser = PydanticOutputParser(pydantic_object=ParseResponse)

def load_environment_variables() -> Dict[str, str]:
    """
    Loads and validates required environment variables, returning them in a dictionary.
    For example, you could load your provider API keys here if not given in config.
    """
    openai_api_key = os.getenv('OPENAI_API_KEY', '')
    # You might also have HF API keys or other environment variables
    return {
        "OPENAI_API_KEY": openai_api_key
    }

def _load_json_file(file_path: str) -> Any:
    """
    Internal helper function to load JSON content from a file.
    """
    path = Path(file_path)
    if not path.exists():
        logger.error(f"File '{file_path}' not found.")
        raise FileNotFoundError(f"File '{file_path}' not found.")

    try:
        with path.open('r', encoding='utf-8') as file:
            data = json.load(file)
        return data
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from '{file_path}': {e}")
        raise
    except IOError as e:
        logger.error(f"I/O error loading file '{file_path}': {e}")
        raise

def _load_text_file(file_path: str, description: str = 'file') -> str:
    """
    Internal helper function to load raw text content from a file.
    """
    path = Path(file_path)
    if not path.exists():
        logger.error(f"{description.capitalize()} file '{file_path}' not found.")
        raise FileNotFoundError(f"{description.capitalize()} file '{file_path}' not found.")

    try:
        with path.open('r', encoding='utf-8') as file:
            content = file.read().strip()
    except OSError as e:
        logger.error(f"Error reading {description} file '{file_path}': {e}")
        raise

    if not content:
        logger.error(f"{description.capitalize()} file '{file_path}' is empty.")
        raise ValueError(f"{description.capitalize()} file '{file_path}' is empty.")

    return content

def load_prompt_file(prompts_folder: str, prompt_file: str, description: str = 'prompt') -> str:
    """
    Loads a prompt from a specified file.
    """
    prompt_path = str(Path(prompts_folder) / prompt_file)
    return _load_text_file(prompt_path, description=description)

def load_config(config_file_path: str) -> ConfigModel:
    """
    Loads and validates the main configuration from a JSON file using Pydantic.
    """
    raw_config = _load_json_file(config_file_path)
    return ConfigModel.model_validate(raw_config)

def load_data_format_config(config_file_path: str) -> DataFormatConfig:
    """
    Loads and validates the data format configuration from a JSON file using Pydantic.
    """
    raw_config = _load_json_file(config_file_path)
    return DataFormatConfig.model_validate(raw_config)

def generate_structured_response(llm: LangChainLLM, prompt: str) -> Dict[str, Any]:
    """
    Example function that uses an LLM to produce structured data validated by Pydantic.
    """
    raw_text = llm.generate(prompt)
    try:
        parsed_obj = parser.parse(raw_text)
        return parsed_obj.dict()
    except ValidationError as ve:
        logger.error(f"Failed to parse structured output: {ve}")
        return {}


===== main.py =====

# main.py

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any

from logging_config import setup_logging  # If you have a custom logging config
from config_schemas import ConfigModel, DataFormatConfig
from utils import (
    load_environment_variables,
    load_config,
    load_data_format_config,
    load_prompt_file
)
from data_handlers import FlexibleDataHandler
from qual_functions import assign_codes_to_meaning_units
from validator import run_validation, replace_nan_with_null  # If you have a validator module

from langchain_llm import LangChainLLM

def main(config: ConfigModel):
    """
    Main function to execute the qualitative coding pipeline.
    """
    setup_logging(
        enable_logging=config.enable_logging,
        logging_level_str=config.logging_level,
        log_to_file=config.log_to_file,
        log_file_path=config.log_file_path
    )
    logger = logging.getLogger(__name__)

    logger.info("Starting the main pipeline.")

    # Stage 1: Environment Setup
    env_vars = load_environment_variables()
    if not config.parse_llm_config.api_key and env_vars.get("OPENAI_API_KEY"):
        config.parse_llm_config.api_key = env_vars["OPENAI_API_KEY"]
    if not config.assign_llm_config.api_key and env_vars.get("HUGGINGFACE_API_KEY"):
        config.assign_llm_config.api_key = env_vars["HUGGINGFACE_API_KEY"]

    parse_instructions = ""
    if config.use_parsing:
        parse_instructions = load_prompt_file(
            config.paths.prompts_folder,
            config.parse_prompt_file,
            description='parse instructions'
        )

    # Load data format configuration
    data_format_config_path = Path(config.paths.config_folder) / 'data_format_config.json'
    data_format_config: DataFormatConfig = load_data_format_config(str(data_format_config_path))

    if config.data_format not in data_format_config:
        logger.error(f"No configuration found for data format: {config.data_format}")
        raise ValueError(f"No configuration found for data format: {config.data_format}")

    format_config = data_format_config[config.data_format]

    # Determine the data file to load
    file_path = Path(config.paths.json_folder) / config.selected_json_file
    if not file_path.exists():
        logger.error(f"Data file '{file_path}' not found.")
        raise FileNotFoundError(f"Data file '{file_path}' not found.")

    # Stage 2: Data Loading & Transform
    # CHANGED: pass context_fields instead of speaker_field
    data_handler = FlexibleDataHandler(
        file_path=str(file_path),
        parse_instructions=parse_instructions,
        completion_model=config.parse_llm_config.model_name,  # Use parse LLM config
        content_field=format_config.content_field,
        context_fields=format_config.context_fields,  # CHANGED
        list_field=format_config.list_field,
        source_id_field=format_config.source_id_field,
        filter_rules=[rule.model_dump() for rule in format_config.filter_rules],
        use_parsing=config.use_parsing,
        speaking_turns_per_prompt=config.speaking_turns_per_prompt,
        thread_count=config.thread_count
    )
    data_df = data_handler.load_data()
    meaning_unit_object_list = data_handler.transform_data(data_df)

    if not meaning_unit_object_list:
        logger.warning("No meaning units to process. Exiting.")
        return

    # Stage 3: Code Assignment
    if config.coding_mode == "deductive":
        coding_instructions = load_prompt_file(
            config.paths.prompts_folder,
            config.deductive_coding_prompt_file,
            description='deductive coding prompt'
        )
        codebase_file = Path(config.paths.codebase_folder) / config.selected_codebase
        if not codebase_file.exists():
            logger.error(f"List of codes file '{codebase_file}' not found.")
            raise FileNotFoundError(f"List of codes file '{codebase_file}' not found.")

        with codebase_file.open('r', encoding='utf-8') as file:
            processed_codes = [json.loads(line) for line in file if line.strip()]

        assign_llm = LangChainLLM(config.assign_llm_config)

        coded_meaning_unit_list = assign_codes_to_meaning_units(
            meaning_unit_list=meaning_unit_object_list,
            coding_instructions=coding_instructions,
            processed_codes=processed_codes,
            codebase=processed_codes,
            completion_model=config.assign_llm_config.model_name,
            context_size=config.context_size,
            meaning_units_per_assignment_prompt=config.meaning_units_per_assignment_prompt,
            context_fields=format_config.context_fields,  # CHANGED
            content_field=format_config.content_field,
            full_speaking_turns=data_handler.full_data.to_dict(orient='records'),
            thread_count=config.thread_count,
            llm_config=config.assign_llm_config
        )
    else:  # inductive
        inductive_coding_prompt = load_prompt_file(
            config.paths.prompts_folder,
            config.inductive_coding_prompt_file,
            description='inductive coding prompt'
        )
        assign_llm = LangChainLLM(config.assign_llm_config)

        coded_meaning_unit_list = assign_codes_to_meaning_units(
            meaning_unit_list=meaning_unit_object_list,
            coding_instructions=inductive_coding_prompt,
            processed_codes=None,
            codebase=None,
            completion_model=config.assign_llm_config.model_name,
            context_size=config.context_size,
            meaning_units_per_assignment_prompt=config.meaning_units_per_assignment_prompt,
            context_fields=format_config.context_fields,  # CHANGED
            content_field=format_config.content_field,
            full_speaking_turns=data_handler.full_data.to_dict(orient='records'),
            thread_count=config.thread_count,
            llm_config=config.assign_llm_config
        )

    # Stage 4: Output Results
    output_folder = Path(config.output_folder)
    output_folder.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file_basename = Path(config.selected_json_file).stem
    output_file_path = output_folder / f"{output_file_basename}_output_{timestamp}.json"

    document_metadata = data_handler.document_metadata

    output_data = {
        "document_metadata": document_metadata,
        "meaning_units": [unit.to_dict() for unit in coded_meaning_unit_list]
    }
    output_data = replace_nan_with_null(output_data)

    with output_file_path.open('w', encoding='utf-8') as outfile:
        json.dump(output_data, outfile, indent=2)

    logger.info(f"Coded meaning units saved to '{output_file_path}'.")

    # Stage 5: Validation
    validation_report_filename = f"{output_file_basename}_validation_report.json"
    validation_report_path = output_folder / validation_report_filename

    run_validation(
        input_file=str(file_path),
        output_file=str(output_file_path),
        report_file=str(validation_report_path),
        similarity_threshold=1.0,
        filter_rules=[rule.model_dump() for rule in format_config.filter_rules],
        input_list_field=format_config.list_field,
        output_list_field='meaning_units',
        text_field=format_config.content_field,
        source_id_field=format_config.source_id_field
    )
    logger.info(f"Validation completed. Report saved to '{validation_report_path}'.")

if __name__ == "__main__":
    config_file_path = 'configs/config.json'
    try:
        config: ConfigModel = load_config(config_file_path)
    except Exception as e:
        print(f"Failed to load configuration: {e}")
        exit(1)

    main(config)


===== qual_functions.py =====

# qual_functions.py

import logging
import json
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from pydantic import BaseModel, ValidationError

from langchain_llm import LangChainLLM
from config_schemas import LLMConfig

logger = logging.getLogger(__name__)

@dataclass
class CodeAssigned:
    code_name: str
    code_justification: str

    def is_valid(self) -> bool:
        return bool(self.code_name and self.code_justification)

@dataclass
class SpeakingTurn:
    source_id: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        metadata = self.metadata.copy()
        metadata.pop('source_id', None)
        return {
            "source_id": self.source_id,
            "content": self.content,
            "metadata": metadata
        }

@dataclass
class MeaningUnit:
    meaning_unit_id: int
    meaning_unit_string: str
    assigned_code_list: List[CodeAssigned] = field(default_factory=list)
    speaking_turn: Optional[SpeakingTurn] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "meaning_unit_id": self.meaning_unit_id,
            "meaning_unit_string": self.meaning_unit_string,
            "assigned_code_list": [code.__dict__ for code in self.assigned_code_list],
            "speaking_turn": self.speaking_turn.to_dict() if self.speaking_turn else None
        }

# -----------------------
# MODELS FOR STRUCTURED OUTPUT
# -----------------------
class CodeAssignedModel(BaseModel):
    code_name: str
    code_justification: str

class CodeAssignmentResponse(BaseModel):
    meaning_unit_id: int
    codeList: List[CodeAssignedModel]

class CodeResponse(BaseModel):
    assignments: List[CodeAssignmentResponse]


def assign_codes_to_meaning_units(
    meaning_unit_list: List[MeaningUnit],
    coding_instructions: str,
    processed_codes: Optional[List[Dict[str, Any]]] = None,
    codebase: Optional[List[Dict[str, Any]]] = None,
    completion_model: str = "gpt-4",
    context_size: int = 2,
    meaning_units_per_assignment_prompt: int = 1,
    # CHANGED: remove speaker_field, add context_fields
    context_fields: Optional[List[str]] = None,  # CHANGED
    content_field: str = 'content',
    full_speaking_turns: Optional[List[Dict[str, Any]]] = None,
    thread_count: int = 1,
    llm_config: Optional[LLMConfig] = None
) -> List[MeaningUnit]:
    """
    Assigns codes to each MeaningUnit, including context. 
    Uses concurrency to parallelize LLM calls.
    """

    if not llm_config:
        logger.error("No LLMConfig provided.")
        return meaning_unit_list

    llm = LangChainLLM(llm_config)

    if full_speaking_turns is None:
        logger.error("full_speaking_turns must be provided for context.")
        return meaning_unit_list

    # Create a mapping from source_id to index for context retrieval
    source_id_to_index = {str(d.get('source_id')): idx for idx, d in enumerate(full_speaking_turns)}

    # Prepare the codebase block
    if processed_codes:
        codes_to_include = codebase if codebase else processed_codes
        unique_codes_strs = set(json.dumps(code, indent=2, sort_keys=True) for code in codes_to_include)
        code_heading = "Full Codebase (all codes with details):"
        codes_block = f"{code_heading}\n{chr(10).join(unique_codes_strs)}\n\n"
    else:
        code_heading = "Guidelines for Inductive Coding:"
        codes_block = (
            f"{code_heading}\nNo predefined codes. Please generate codes based on the following guidelines.\n\n"
        )

    # We store results from each batch in a dict to avoid collisions
    batch_results_map: Dict[int, List[CodeAssigned]] = {}

    def process_batch(start_idx: int) -> Dict[int, List[CodeAssigned]]:
        """
        Process a batch of meaning units, returning a dict so we can safely
        merge results after threads finish.
        """
        local_result: Dict[int, List[CodeAssigned]] = {}
        batch = meaning_unit_list[start_idx:start_idx + meaning_units_per_assignment_prompt]
        if not batch:
            return local_result

        # Build a combined prompt for this batch
        full_prompt = f"{coding_instructions}\n\n{codes_block}"

        for unit in batch:
            unit_context = ""
            source_id = unit.speaking_turn.source_id
            unit_idx = source_id_to_index.get(source_id)

            # Retrieve context_speaking_turns up to `context_size` 
            if unit_idx is not None:
                start_context_idx = max(0, unit_idx - (context_size - 1))
                end_context_idx = unit_idx + 1
                context_speaking_turns = full_speaking_turns[start_context_idx:end_context_idx]
                for st in context_speaking_turns:
                    st_source_id = str(st.get('source_id'))
                    # CHANGED: Instead of a single speaker_field, loop through context_fields
                    context_info_lines = []
                    if context_fields:
                        for fld in context_fields:
                            val = st.get(fld, "Unknown")
                            context_info_lines.append(f"{fld}: {val}")
                    else:
                        # fallback if no context fields
                        context_info_lines.append("No context fields defined.")

                    # The main textual content
                    content_val = st.get(content_field, "")
                    context_block = f"ID: {st_source_id}\n" + "\n".join(context_info_lines) + f"\n{content_val}\n\n"
                    unit_context += context_block

            # For the current excerpt, also gather context from the meaning unit's speaking turn
            current_context_lines = []
            if context_fields and unit.speaking_turn:
                for fld in context_fields:
                    val = unit.speaking_turn.metadata.get(fld, "Unknown")
                    current_context_lines.append(f"{fld}: {val}")

            current_unit_excerpt = f"Quote: {unit.meaning_unit_string}\n\n"

            full_prompt += (
                f"Contextual Excerpts for Meaning Unit ID {unit.meaning_unit_id}:\n{unit_context}\n"
                f"Current Excerpt For Coding (Meaning Unit ID {unit.meaning_unit_id}):\n"
                + "\n".join(current_context_lines) + "\n"
                + f"{current_unit_excerpt}"
            )

        if processed_codes:
            full_prompt += "**Apply codes exclusively to the excerpt(s) provided above.**\n\n"
        else:
            full_prompt += "**Generate codes based on the excerpt(s) provided above using the guidelines.**\n\n"

        # Attempt structured generation
        try:
            code_response = llm.structured_generate(full_prompt, CodeResponse)
        except Exception as e:
            logger.warning(f"Structured generation failed for batch {start_idx}, using fallback empty assignment. Error: {e}")
            code_response = CodeResponse(assignments=[])

        for assignment in code_response.assignments:
            assigned_codes_list: List[CodeAssigned] = []
            for code_item in assignment.codeList:
                assigned_codes_list.append(CodeAssigned(
                    code_name=code_item.code_name,
                    code_justification=code_item.code_justification
                ))
            local_result[assignment.meaning_unit_id] = assigned_codes_list

        return local_result

    from math import ceil
    total_batches = ceil(len(meaning_unit_list) / meaning_units_per_assignment_prompt)

    with ThreadPoolExecutor(max_workers=thread_count) as executor:
        futures = []
        for i in range(0, len(meaning_unit_list), meaning_units_per_assignment_prompt):
            futures.append(executor.submit(process_batch, i))

        for future in as_completed(futures):
            batch_dict = future.result()
            for mu_id, code_list in batch_dict.items():
                batch_results_map[mu_id] = code_list

    # Update meaning_unit_list with assigned codes
    for mu in meaning_unit_list:
        if mu.meaning_unit_id in batch_results_map:
            mu.assigned_code_list = batch_results_map[mu.meaning_unit_id]

    return meaning_unit_list


===== langchain_llm.py =====

# langchain_llm.py

from typing import Any, Type, Union, Dict
from langchain_openai import ChatOpenAI
from huggingface_hub import InferenceClient
from pydantic import BaseModel
from config_schemas import LLMConfig, ProviderEnum

"""
CHANGES:
1) Added a `structured_generate` method that attempts to call `with_structured_output(...)`
   if using an OpenAI model that supports it. Otherwise, it falls back to a manual JSON approach.
"""

class LangChainLLM:
    """
    A simple wrapper around LangChain LLMs to abstract provider details (OpenAI / Hugging Face).
    """
    def __init__(self, config: LLMConfig):
        self.config = config
        self.provider = config.provider
        self.client = self._initialize_client(config)

    def _initialize_client(self, config: LLMConfig) -> Any:
        if self.provider == ProviderEnum.OPENAI:
            # ChatOpenAI from langchain_openai, passing in OpenAI API key
            return ChatOpenAI(
                model_name=config.model_name,
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                openai_api_key=config.api_key
            )
        elif self.provider == ProviderEnum.HUGGINGFACE:
            # Hugging Face Serverless Inference API
            if not config.api_key:
                raise ValueError("Hugging Face API key is required for serverless inference.")
            return InferenceClient(
                model=config.model_name,
                token=config.api_key
            )
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

    def generate(self, prompt: str) -> str:
        """
        Generate text completion from the underlying client.
        """
        try:
            if self.provider == ProviderEnum.OPENAI:
                # Generate using ChatOpenAI
                response = self.client.generate([prompt])
                return response.generations[0][0].text
            elif self.provider == ProviderEnum.HUGGINGFACE:
                # Generate using Hugging Face InferenceClient
                response = self.client.chat.completions.create(prompt)
                return response.choices[0].message
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
        except Exception as e:
            # Handle exceptions and provide meaningful error messages
            raise RuntimeError(f"An error occurred during text generation: {e}")

    # ---------------------------------------------------------------
    # NEW: structured_generate method using with_structured_output
    # ---------------------------------------------------------------
    def structured_generate(self, prompt: str, schema: Type[BaseModel]) -> BaseModel:
        """
        Attempt to generate structured data from the model, adhering to the given Pydantic schema.
        - If using OpenAI with a model that supports tool/function calling or JSON Mode, 
          we leverage .with_structured_output().
        - Otherwise, we fallback to manual JSON parsing of the LLM output.
        """
        if self.provider in {ProviderEnum.OPENAI, ProviderEnum.HUGGINGFACE}:
            # Attempt structured output
            # NOTE: Not all OpenAI models support .with_structured_output(). 
            #       This example assumes your chosen model does.
            try:
                structured_llm = self.client.with_structured_output(schema)
                result = structured_llm.invoke(prompt)
                return result  # result is an instance of `schema`
            except AttributeError:
                # Fallback to manual
                raw_text = self.generate(prompt)
                return self._manual_parse(raw_text, schema)
        else:
            # prompt for JSON and parse
            raw_text = self.generate(prompt)
            return self._manual_parse(raw_text, schema)

    def _manual_parse(self, raw_text: str, schema: Type[BaseModel]) -> BaseModel:
        """
        Parse the raw text as JSON and validate against the given schema.
        If parsing fails, return an empty instance of the schema.
        """
        try:
            data = self._extract_first_json(raw_text)
            return schema.model_validate(data)
        except Exception:
            # In case of failure, return an empty instance
            return schema()  # type: ignore

    def _extract_first_json(self, text: str) -> Dict[str, Any]:
        """
        Very naive approach to extract the first valid JSON block from text.
        """
        import json, re
        pattern = r"\{(?:[^{}]|(?R))*\}"
        matches = re.findall(pattern, text)
        for match in matches:
            try:
                return json.loads(match)
            except json.JSONDecodeError:
                continue
        # If no match or all fail
        return {}


===== data_handlers.py =====

# data_handlers.py

import json
import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

from qual_functions import MeaningUnit, SpeakingTurn
from config_schemas import ProviderEnum, LLMConfig
from langchain_llm import LangChainLLM

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# NEW: Pydantic models for structured parse responses
# ------------------------------------------------------------------
from pydantic import BaseModel, ValidationError
from typing import List


class ParseUnit(BaseModel):
    """
    Represents a single meaning unit (smaller chunk) after parsing.
    """
    source_id: str
    quote: str


class FullParseResponse(BaseModel):
    """
    Represents the entire structured output from the parsing step.
    """
    parse_list: List[ParseUnit]


class FlexibleDataHandler:
    def __init__(
        self,
        file_path: str,
        parse_instructions: str,
        completion_model: str,
        content_field: str,
        # CHANGED: replaced speaker_field with context_fields
        context_fields: Optional[List[str]] = None,  # CHANGED
        list_field: Optional[str] = None,
        source_id_field: Optional[str] = None,
        filter_rules: Optional[List[Dict[str, Any]]] = None,
        use_parsing: bool = True,
        speaking_turns_per_prompt: int = 1,
        thread_count: int = 1
    ):
        self.file_path = file_path
        self.parse_instructions = parse_instructions
        self.completion_model = completion_model
        self.content_field = content_field
        self.context_fields = context_fields  # CHANGED
        self.list_field = list_field
        self.source_id_field = source_id_field
        self.filter_rules = filter_rules
        self.use_parsing = use_parsing
        self.speaking_turns_per_prompt = speaking_turns_per_prompt
        self.thread_count = thread_count
        self.document_metadata = {}  # Store document-level metadata
        self.full_data = None
        self.filtered_out_source_ids: Set[str] = set()

        # ------------------------------------------------------------------
        # NEW: Initialize LangChainLLM for parsing (if needed)
        # ------------------------------------------------------------------
        self.llm = None
        if self.use_parsing:
            try:
                # Build an LLMConfig (example: defaulting to OpenAI with a 0.2 temperature)
                self.llm_config = LLMConfig(
                    provider=ProviderEnum.OPENAI,
                    model_name=self.completion_model,
                    temperature=0.2,
                    max_tokens=16000,
                    api_key=os.getenv('OPENAI_API_KEY', '')  # or however you manage API keys
                )
                self.llm = LangChainLLM(self.llm_config)
            except Exception as e:
                logger.warning(
                    f"Could not initialize LangChainLLM with model={self.completion_model}: {e}"
                )
                self.llm = None

    def load_data(self) -> pd.DataFrame:
        """
        Loads data from the JSON file into a DataFrame and applies filter rules if any.
        """
        try:
            with Path(self.file_path).open('r', encoding='utf-8') as file:
                raw_data = json.load(file)
            logger.debug(f"Loaded data from '{self.file_path}'.")
        except (OSError, json.JSONDecodeError) as e:
            logger.error(f"Failed to load or parse data from '{self.file_path}': {e}")
            raise

        # Determine how the data is structured
        if isinstance(raw_data, list):
            self.document_metadata = {}
            content_list = raw_data
        elif isinstance(raw_data, dict):
            if self.list_field:
                self.document_metadata = {
                    k: v for k, v in raw_data.items() if k != self.list_field
                }
                content_list = raw_data.get(self.list_field, [])
                if not content_list:
                    logger.error(f"No content found under the list_field '{self.list_field}'.")
                    raise ValueError(f"No content found under the list_field '{self.list_field}'.")
            else:
                self.document_metadata = {
                    k: v for k, v in raw_data.items() if not isinstance(v, list)
                }
                content_list = [v for v in raw_data.values() if isinstance(v, list)]
                if content_list:
                    content_list = content_list[0]
                else:
                    logger.error("No list of content items found in the data.")
                    raise ValueError("No list of content items found in the data.")
        else:
            logger.error(f"Unexpected data format in '{self.file_path}'. Expected dict or list.")
            raise ValueError(f"Unexpected data format in '{self.file_path}'.")

        data = pd.DataFrame(content_list)

        # Ensure we have a source_id
        if self.source_id_field and self.source_id_field in data.columns:
            data['source_id'] = data[self.source_id_field].astype(str)
        else:
            data['source_id'] = [f"auto_{i}" for i in range(len(data))]

        self.full_data = data.copy()
        all_source_ids = set(data['source_id'])

        # Apply filter rules if any
        if self.filter_rules:
            mask = pd.Series(True, index=data.index)
            for rule in self.filter_rules:
                field = rule.get('field')
                operator = rule.get('operator', 'equals')
                value = rule.get('value')

                if field not in data.columns:
                    logger.warning(f"Field '{field}' not found in data. Skipping filter rule.")
                    continue

                if operator == 'equals':
                    mask &= (data[field] == value)
                elif operator == 'not_equals':
                    mask &= (data[field] != value)
                elif operator == 'contains':
                    mask &= data[field].astype(str).str.contains(str(value), na=False, regex=False)
                else:
                    logger.warning(f"Operator '{operator}' is not supported. Skipping this filter rule.")

            data = data[mask]
            logger.debug(f"Data shape after applying filter rules: {data.shape}")

        # Identify filtered out source_ids
        filtered_source_ids = all_source_ids - set(data['source_id'])
        self.filtered_out_source_ids = filtered_source_ids
        logger.debug(f"Data shape after loading: {data.shape}")

        return data

    def _run_langchain_parse_chunk(
        self,
        speaking_turns: List[Dict[str, Any]],
        prompt: str
    ) -> List[Dict[str, str]]:
        """
        Uses LangChainLLM.structured_generate() to parse a chunk of speaking_turns into smaller meaning units.
        Returns a list of dicts with keys "source_id" and "parsed_text".
        """

        if not self.llm:
            logger.error("LLM was not initialized; cannot parse.")
            return []

        # Build the combined prompt (system role + user instructions + data)
        system_content = (
            "You are a qualitative research assistant that breaks down multiple speaking turns "
            "into smaller meaning units based on given instructions."
        )

        combined_prompt = f"""
System instructions:
{system_content}

User instructions:
{prompt}

Speaking Turns (JSON):
{json.dumps(speaking_turns, indent=2)}
        """

        try:
            parsed_response: FullParseResponse = self.llm.structured_generate(
                combined_prompt,
                FullParseResponse
            )

            if not parsed_response or not isinstance(parsed_response, FullParseResponse):
                logger.error("Parsed output is not an instance of FullParseResponse.")
                return []

            if not parsed_response.parse_list:
                logger.error("Parsed output is empty or missing 'parse_list'.")
                return []

            results = []
            for unit in parsed_response.parse_list:
                results.append({
                    "source_id": unit.source_id,
                    "parsed_text": unit.quote
                })

            return results

        except ValidationError as ve:
            logger.error(f"Validation error while parsing chunk: {ve}")
            return []
        except Exception as e:
            logger.error(f"An error occurred while parsing chunk: {e}")
            return []

    def _parse_chunk_of_data(
        self,
        chunk_data: pd.DataFrame,
        parse_instructions: str,
        batch_index: int
    ) -> Tuple[int, List[Dict[str, str]]]:
        """
        Helper method for parsing a chunk of data.
        Returns a tuple of (batch_index, parsed_units).
        """
        speaking_turns_dicts = []
        for _, record in chunk_data.iterrows():
            speaking_turns_dicts.append({
                "source_id": str(record['source_id']),
                "content": record.get(self.content_field, ""),
                "metadata": record.drop(labels=[self.content_field], errors='ignore').to_dict()
            })

        parsed_units = self._run_langchain_parse_chunk(
            speaking_turns=speaking_turns_dicts,
            prompt=parse_instructions
        )

        return (batch_index, parsed_units)

    def transform_data(self, data: pd.DataFrame) -> List[MeaningUnit]:
        """
        Transforms data into MeaningUnit objects, optionally using LLM-based parsing.
        If parsing is off, treat each row as a single meaning unit.
        Otherwise, parse in batches using concurrency.
        """
        meaning_units: List[MeaningUnit] = []

        if not self.use_parsing:
            # No parsing needed
            meaning_unit_id_counter = 1
            for _, record in data.iterrows():
                content = record.get(self.content_field, "")
                metadata = record.drop(labels=[self.content_field], errors='ignore').to_dict()
                source_id = str(record['source_id'])

                speaking_turn = SpeakingTurn(
                    source_id=source_id,
                    content=content,
                    metadata=metadata
                )
                mu = MeaningUnit(
                    meaning_unit_id=meaning_unit_id_counter,
                    meaning_unit_string=content,
                    speaking_turn=speaking_turn
                )
                meaning_units.append(mu)
                meaning_unit_id_counter += 1

            logger.debug(f"Transformed data (no parsing) into {len(meaning_units)} meaning units.")
            return meaning_units

        # PARSING is ON
        chunked_data = [
            data.iloc[i: i + self.speaking_turns_per_prompt]
            for i in range(0, len(data), self.speaking_turns_per_prompt)
        ]

        all_parsed_results: List[Tuple[int, List[Dict[str, str]]]] = []
        with ThreadPoolExecutor(max_workers=self.thread_count) as executor:
            futures = {}
            for idx, chunk in enumerate(chunked_data):
                future = executor.submit(
                    self._parse_chunk_of_data,
                    chunk_data=chunk,
                    parse_instructions=self.parse_instructions,
                    batch_index=idx
                )
                futures[future] = idx

            for future in as_completed(futures):
                batch_index, parsed_list = future.result()
                all_parsed_results.append((batch_index, parsed_list))

        # Sort the results based on batch_index to maintain order
        all_parsed_results.sort(key=lambda x: x[0])

        meaning_unit_id_counter = 1
        for _, parsed_list in all_parsed_results:
            for item in parsed_list:
                sid = item["source_id"]
                parsed_text = item["parsed_text"]
                record = data[data['source_id'] == sid].iloc[0]
                speaking_turn = SpeakingTurn(
                    source_id=sid,
                    content=record.get(self.content_field, ""),
                    metadata=record.drop(labels=[self.content_field], errors='ignore').to_dict()
                )
                mu = MeaningUnit(
                    meaning_unit_id=meaning_unit_id_counter,
                    meaning_unit_string=parsed_text,
                    speaking_turn=speaking_turn
                )
                meaning_units.append(mu)
                meaning_unit_id_counter += 1

        logger.debug(f"Transformed data (with parsing) into {len(meaning_units)} meaning units.")
        return meaning_units
